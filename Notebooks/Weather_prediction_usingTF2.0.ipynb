{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather data Prediction using TF2.0\n",
    "\n",
    "### Importing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data.\n",
    "\n",
    "The data is downloaded from [source](https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip). The data can be downloaded and placed in the project or it can be done directly using following commands:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "zip_path = tf.keras.utils.get_file(\n",
    "    origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "    fname='jena_climate_2009_2016.csv.zip',\n",
    "    extract=True)\n",
    "csv_path, _ = os.path.splitext(zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, 'jena_climate_2009_2016.csv'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic EDA with preprocessing and making the data ready for input to model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice [start:stop:step], from index 5 taking every 6th record (sub sampling data as the values are for eery 10mins converting data to every hour values)\n",
    "df = df[5::6]\n",
    "\n",
    "#storing date time values in a separate variable for future processing\n",
    "date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cols = ['T (degC)', 'p (mbar)', 'rho (g/m**3)']\n",
    "plot_features = df[plot_cols]\n",
    "plot_features.index = date_time\n",
    "_ = plot_features.plot(subplots= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features = df[plot_cols][:480]\n",
    "plot_features.index = date_time[:480]\n",
    "\n",
    "_ = plot_features.plot(subplots = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(date_time,df['wv (m/s)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select garbage -9999.0 values of wv and max_wv and replacing them with 0\n",
    "wv = df['wv (m/s)']\n",
    "bad_wv = wv== -9999.0\n",
    "wv[bad_wv] = 0.0 #replacing low NA values of wv with 0\n",
    "\n",
    "max_wv = df['max. wv (m/s)']\n",
    "bad_max_wv = max_wv == -9999.0\n",
    "max_wv[bad_max_wv] = 0.0\n",
    "\n",
    "#sanity check\n",
    "df['wv (m/s)'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found certain useful features after doing EDA on the data. Adding those features to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv= df.pop('wv (m/s)')\n",
    "max_wv = df.pop('max. wv (m/s)')\n",
    "\n",
    "# convert to radians\n",
    "wd_rad = df.pop('wd (deg)') * np.pi / 180\n",
    "\n",
    "# Calculate the x and y components of the wind velocity\n",
    "df['Wx'] = wv * np.cos(wd_rad)\n",
    "df['Wy'] = wv * np.sin(wd_rad)\n",
    "\n",
    "# Calculate the x and y components of max wind velocity\n",
    "df['max Wx'] = max_wv * np.cos(wd_rad)\n",
    "df['max Wy'] = max_wv * np.sin(wd_rad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting date time into multiple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_sec = date_time.map(datetime.datetime.timestamp)\n",
    "\n",
    "day = 24 * 60 * 60\n",
    "year = (365.2425) * day\n",
    "\n",
    "df['Day sin'] = np.sin(timestamp_sec * (2 * np.pi / day))\n",
    "df['Day cos'] = np.cos(timestamp_sec * (2 * np.pi / day))\n",
    "df['Year sin'] = np.sin(timestamp_sec * (2 * np.pi / year))\n",
    "df['Year cos'] = np.cos(timestamp_sec * (2 * np.pi / year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting data into train:val:test in 70:20:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of column names and their indices\n",
    "column_indices = {name: i for i, name in enumerate(df.columns)}\n",
    "\n",
    "n = len(df)\n",
    "#  Splitting the dataset with a 70:20:10 split\n",
    "train_df = df[0:int(n*0.7)]\n",
    "val_df = df[int(n*0.7):int(n*0.9)]\n",
    "test_df = df[int(n*0.9):]\n",
    "# Number of features in our dataset\n",
    "num_features = df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating data window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowGenerator():\n",
    "    def __init__(self, input_width, label_width, shift, train_df=train_df, val_df=val_df, test_df=test_df, label_columns=None):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.val_df = val_df\n",
    "        \n",
    "        self.label_columns = label_columns\n",
    "        if label_columns is not None:\n",
    "            self.label_column_indices = {name: i for i, name in enumerate(label_columns)}\n",
    "        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n",
    "            \n",
    "        #Window parameters.\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width \n",
    "        self.shift = shift\n",
    "            \n",
    "        self.total_window_size = input_width + shift\n",
    "            \n",
    "        self.input_slice = slice(0, input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "            \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Total window size: {self.total_window_size}',\n",
    "            f'Input indices: {self.input_indices}',\n",
    "            f'Label indices: {self.label_indices}',\n",
    "            f'Label column name(s): {self.label_columns}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check of the function\n",
    "\n",
    "w1 = WindowGenerator(input_width = 6, label_width =  1, shift = 1, label_columns = ['T (degC)'])\n",
    "print(f'First Window: \\n{w1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
